{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c00b45-c571-4438-be9b-60f80e8be054",
   "metadata": {},
   "source": [
    "# Fie Upon Thee, Autocorrect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54dee98-c213-4197-b2eb-8184fa70d7ea",
   "metadata": {},
   "source": [
    "<img src=\"img/shakespeare.jpg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd8904-0e5d-41bc-98b8-6d56c14eb7db",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d2eaf8-e86f-4f3d-8c15-38b5597ccfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30075a76-6d43-40da-adcd-cb14acc3f3ce",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046cc6b9-7f8f-4243-b018-5c6a34e81957",
   "metadata": {},
   "source": [
    "## Dataset: the complete works of Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9f28d-7ca0-4a42-a8e2-3b7780f914f8",
   "metadata": {},
   "source": [
    "This used to be a big dataset, used to illustrate large storage devices, like in [this definition of CD-ROM](https://vintageapple.org/apple_ii/pdf/Apple_IIGS__Ownwers_Guide_1986.pdf) from 1986:\n",
    "\n",
    "<img src=\"img/shakespeare-a-big-dataset.png\" width=\"600\">\n",
    "\n",
    "Now it's small enough to easily load in JupyterLite but is still big enough to be interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e5f10-bc12-46fc-ba20-de8f8b8a329e",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8767d-18fb-4b0c-907e-eab6720f13d3",
   "metadata": {},
   "source": [
    "This file comes from Project Gutenberg, [ebook #100](https://www.gutenberg.org/ebooks/100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e692285-26fe-4e53-9cbd-95ea094627c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/shakespeare.txt\") as file:\n",
    "    corpus = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2cd8d-2291-4550-b468-56bc01368edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59238d63-7e71-4976-b412-4df3c5b5f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus) / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79440957-226f-4d21-add6-ee5ec85ead69",
   "metadata": {},
   "source": [
    "5.36 MB (a whole laser disk, apparently)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69243038-8b18-4bf1-95cf-10846f219be9",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41439cb7-f5f1-466c-9006-966543672fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[100000:101000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0ba72c-2114-44d3-a833-89e824cf1f61",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626cb458-2dfd-4c5d-8e01-6fe22e7439a5",
   "metadata": {},
   "source": [
    "What distinct characters does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc451885-2555-4260-a0a2-6083e6f9d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84875aef-4b40-4c1d-aead-e04e847fcca3",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117f359c-752f-4d3a-96e6-61790753fd59",
   "metadata": {},
   "source": [
    "## How often is \"t\" followed by \"h\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02184a24-fc46-4dbe-94b1-14484447814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_character = []\n",
    "next_character = []\n",
    "for i in range(len(corpus) - 1):\n",
    "    first_character.append(corpus[i])\n",
    "    next_character.append(corpus[i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d05d4c-6609-4ac0-9227-22ef581be17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_character[100414:100439]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ebf782-ccb3-4d5f-83ca-23ce4ece03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_character[100414:100439]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5805a-68d8-4d41-960a-4a1847dbd81c",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563e70f-ec58-44fc-872a-5ff978a71df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.crosstab(first_character, next_character, rownames=[\"first\"], colnames=[\"next\"])\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4603180-edd3-4c78-b211-c54abe9c9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_columns = pairs[pairs.sum(axis=0).sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d055a7-b3b5-4784-9646-dca076591d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_both = sorted_by_columns.loc[sorted_by_columns.sum(axis=1).sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250301d0-4d0a-49da-b6aa-dada4039356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7268d0-8d10-476e-bfac-c7b1e8650951",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "matrix = ax.matshow(sorted_by_both.values)\n",
    "\n",
    "ax.set_xticks(range(len(sorted_by_both.index)), sorted_by_both.index)\n",
    "ax.set_yticks(range(len(sorted_by_both.columns)), sorted_by_both.columns)\n",
    "\n",
    "ax.set_xlim(-0.5, 25.5)\n",
    "ax.set_ylim(-0.5, 25.5)\n",
    "ax.set_ylabel(\"first character\")\n",
    "ax.set_xlabel(\"next character\")\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2045102-6a91-4d9a-8fcb-d57466656506",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36858464-3be9-41e0-acf2-9f7e392e9463",
   "metadata": {},
   "source": [
    "The bright spots are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f193d-729b-445f-8b1b-181dd7afe69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.loc[\"e\", \" \"]   # e followed by space (at the end of a word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5903a8-3e8a-416a-b0cd-9e496946198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.loc[\" \", \"t\"]   # space followed by t (at the beginning of a word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb444e91-2654-4648-9132-a8a97a2f00b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.loc[\"t\", \"h\"]   # t followed by h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82fa19-da0a-4f82-b7c6-5f3312dedade",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.loc[\".\", \" \"]   # period followed by space (at the end of a sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b83f40-5435-46aa-94f0-27d5341a44ac",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650cac4-b72d-4fc4-9501-30839d99505d",
   "metadata": {},
   "source": [
    "## From sequence of characters to sequence of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea2662-9030-4454-9de5-ff6d7f90c12c",
   "metadata": {},
   "source": [
    "We could build a per-letter autocomplete algorithm that would see \"t\" and suggest \"h\", but it wouldn't produce interesting text.\n",
    "\n",
    "It gets more interesting if we do this at the word level.\n",
    "\n",
    "The first step of **parsing**, an analysis of human-readable text, is **tokenizing** the input: turning raw characters into **tokens**.\n",
    "\n",
    "Why \"tokens\" and not \"words\"? Some of our tokens will be punctuation marks, so that when your autocomplete algorithm sees `hark` it can suggest a token like `!`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c15f17c-4e07-4a0e-8668-4bc683d34404",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03dfc1-ed92-430c-b384-c3a5f967d654",
   "metadata": {},
   "source": [
    "**Regular expressions** or **regex** is a mini-language for recognizing strings and parts of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b332a30-9e4c-4fec-b5f8-71d565073390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c72543-5ca1-4442-af57-e5638ea68869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#               \"‚Äô\" for \"thou be‚Äôst\" and \"-\" for \"a-foot\"       multi-digit number as a token\n",
    "#                olden-time (and French) letters     ü°ì    \"&c.\"  ü°ì  \"+\" matches sequences of at least 1 character\n",
    "#        capital and lowercase letters       ü°ì       ü°ì     ü°ì     ü°ì  ü°ì   match any single character that is not (^) a space\n",
    "#                                  ü°ì         ü°ì       ü°ì     ü°ì     ü°ì  ü°ì   ü°ì\n",
    "recognize_token = re.compile(\"([A-Za-z√Ä√Ü√á√â√†√¢√¶√ß√®√©√™√´√Æ≈ì‚Äô-]+|&c\\.|[0-9]+|[^ ])\")\n",
    "#                            ü°ë                          ü°ë    ü°ë      ü°ë\n",
    "#              \"(\" and \")\" form a group               \"|\" means \"or\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a3774-ef34-4793-960a-636149cca0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognize_token.findall(\"Dost thou be‚Äôst a tokenized sen-t√´nce, &c.?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968d6b1-ea41-42d0-af3a-eea5897d58cf",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94cc2a7-11a7-4d1b-84ed-eb7aadbf0c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in recognize_token.finditer(corpus):\n",
    "    token = match.group(0)\n",
    "    print(repr(token))\n",
    "    if token == \".\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a99f24-39dc-4924-acae-b4a25d633777",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a0ae9-7163-4ea9-a2a2-562decb6413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for match in recognize_token.finditer(corpus):\n",
    "    tokens.append(match.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335e092-e7b4-49be-8a0d-99b63f799a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77627f43-4869-4e49-9503-8ef99f936841",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88638040-b7f2-4987-ab0a-0941facdba85",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac71590-b514-44d6-89ac-74b88309fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da30d44-72ce-4754-9471-a72b57bfc869",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26e7b0-d199-439f-947a-07ed1fa653f8",
   "metadata": {},
   "source": [
    "Instead of a 103√ó103 table of \"first\", \"next\" pairs, this would be a 36775√ó36775 table. Too big!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bcdc5-531d-464e-b9e8-e15149f96d62",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537db7ff-6fa2-493c-bc51-bc8e1c1dc52e",
   "metadata": {},
   "source": [
    "## SQL, the language of table manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2311070e-d321-4297-a6d5-d2d42c771bc4",
   "metadata": {},
   "source": [
    "Just as **regular expression** is a mini-language that we can call from Python to handle strings, **SQL** is a language to deal with tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acab08f-6311-44b4-ab17-7c884a248157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc085d0-2804-47f0-9c08-eea3099a6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(\":memory:\")\n",
    "db.execute(\"CREATE TABLE works(title TEXT, type TEXT, characters INTEGER, year_low INTEGER, year_high INTEGER)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece55759-ceb4-41d4-850a-d441feda868a",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ebf05-0c8e-4602-9252-27a45fdee0b7",
   "metadata": {},
   "source": [
    "Some data to feed into the table, in Python format (lists, strings, and numbers).\n",
    "\n",
    "_(Don't assume these numbers are correct; I got them from ChatGPT.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb6f63-b413-46ad-9d6b-eee7b77fe1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_python = [\n",
    "    [\"The Sonnets\", \"poetry\", None, 1609, 1609],\n",
    "    [\"All‚Äôs Well that Ends Well\", \"comedy\", 23, 1604, 1605],\n",
    "    [\"The Tragedy of Antony and Cleopatra\", \"tragedy\", 42, 1606, 1606],\n",
    "    [\"As You Like It\", \"comedy\", 27, 1599, 1600],\n",
    "    [\"The Comedy of Errors\", \"comedy\", 18, 1594, 1594],\n",
    "    [\"The Tragedy of Coriolanus\", \"tragedy\", 30, 1608, 1608],\n",
    "    [\"Cymbeline\", \"mixed\", 20, 1609, 1610],\n",
    "    [\"The Tragedy of Hamlet, Prince of Denmark\", \"tragedy\", 30, 1599, 1601],\n",
    "    [\"The First Part of King Henry the Fourth\", \"history\", 25, 1596, 1597],\n",
    "    [\"The Second Part of King Henry the Fourth\", \"history\", 25, 1597, 1598],\n",
    "    [\"The Life of King Henry the Fifth\", \"history\", 30, 1599, 1599],\n",
    "    [\"The First Part of Henry the Sixth\", \"history\", 40, 1590, 1592],\n",
    "    [\"The Second Part of King Henry the Sixth\", \"history\", 30, 1590, 1591],\n",
    "    [\"The Third Part of King Henry the Sixth\", \"history\", 30, 1591, 1591],\n",
    "    [\"King Henry the Eighth\", \"history\", 30, 1612, 1613],\n",
    "    [\"The Life and Death of King John\", \"history\", 20, 1596, 1596],\n",
    "    [\"The Tragedy of Julius Caesar\", \"tragedy\", 40, 1599, 1599],\n",
    "    [\"The Tragedy of King Lear\", \"tragedy\", 20, 1605, 1606],\n",
    "    [\"Love‚Äôs Labour‚Äôs Lost\", \"comedy\", 23, 1594, 1595],\n",
    "    [\"The Tragedy of Macbeth\", \"tragedy\", 20, 1606, 1606],\n",
    "    [\"Measure for Measure\", \"comedy\", 20, 1603, 1604],\n",
    "    [\"The Merchant of Venice\", \"comedy\", 22, 1596, 1597],\n",
    "    [\"The Merry Wives of Windsor\", \"comedy\", 24, 1597, 1597],\n",
    "    [\"A Midsummer Night‚Äôs Dream\", \"comedy\", 21, 1595, 1596],\n",
    "    [\"Much Ado About Nothing\", \"comedy\", 23, 1598, 1599],\n",
    "    [\"The Tragedy of Othello, the Moor of Venice\", \"tragedy\", 21, 1603, 1604],\n",
    "    [\"Pericles, Prince of Tyre\", \"late romance\", 20, 1607, 1608],\n",
    "    [\"King Richard the Second\", \"history\", 20, 1595, 1595],\n",
    "    [\"King Richard the Third\", \"history\", 30, 1592, 1593],\n",
    "    [\"The Tragedy of Romeo and Juliet\", \"tragedy\", 20, 1595, 1595],\n",
    "    [\"The Taming of the Shrew\", \"comedy\", 16, 1590, 1592],\n",
    "    [\"The Tempest\", \"late romance\", 12, 1610, 1611],\n",
    "    [\"The Life of Timon of Athens\", \"tragedy\", 20, 1605, 1606],\n",
    "    [\"The Tragedy of Titus Andronicus\", \"tragedy\", 25, 1591, 1592],\n",
    "    [\"Troilus and Cressida\", \"mixed\", 30, 1601, 1602],\n",
    "    [\"Twelfth Night; or, What You Will\", \"comedy\", 18, 1601, 1602],\n",
    "    [\"The Two Gentlemen of Verona\", \"comedy\", 20, 1589, 1593],\n",
    "    [\"The Two Noble Kinsmen\", \"comedy\", 20, 1613, 1614],\n",
    "    [\"The Winter‚Äôs Tale\", \"comedy\", 21, 1609, 1611],\n",
    "    [\"A Lover‚Äôs Complaint\", \"poetry\", None, 1609, 1609],\n",
    "    [\"The Passionate Pilgrim\", \"poetry\", None, 1599, 1599],\n",
    "    [\"The Phoenix and the Turtle\", \"poetry\", None, 1601, 1601],\n",
    "    [\"The Rape of Lucrece\", \"poetry\", 2, 1594, 1594],\n",
    "    [\"Venus and Adonis\", \"poetry\", 2, 1593, 1593],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf5f4f-6520-440c-8dd8-e8c6157a6774",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf176f46-45bf-4fc3-b5de-bb9986aaee4b",
   "metadata": {},
   "source": [
    "This `INSERT INTO` SQL command has five `?`s that get filled with each row of the data from Python.\n",
    "\n",
    "After preparing the command, `db.commit()` tells the database engine to run it (fast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc7f00f-6b04-4a12-884c-124e69b0733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.executemany(\"INSERT INTO works VALUES(?, ?, ?, ?, ?)\", data_in_python)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b4554-621d-41b6-9357-bb5f4107d8f6",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa97843-3aca-42e3-9654-71b28ce77a35",
   "metadata": {},
   "source": [
    "Although SQL will be more computationally efficient for what we want to do, it doesn't have a convenient way to print out and look at a table, so we'll dump it into Pandas when we want to view it.\n",
    "\n",
    "`SELECT` means select columns (not rows), and `*` means \"everything\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a30ef5-7ddc-4958-a05d-8934291e74d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT * FROM works\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c514d-1aa8-447c-8c91-676a60e499b3",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ca1f5-6099-435d-98e7-bdfe75b29e69",
   "metadata": {},
   "source": [
    "SQL has mathematical manipulations, such as `year_uncertainty` = `year_high - year_low`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0787c-c2a4-40fd-a56d-6eb204283b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT title, year_high - year_low AS year_uncertainty FROM works\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52012a34-7b9b-4cf0-a6f8-6e37ade1917e",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ed47b-c6a7-490d-b09e-954a7bbf4441",
   "metadata": {},
   "source": [
    "`WHERE` selects rows (not columns) by value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72974e1-68a7-4f8a-846f-46a4c899e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT * FROM works WHERE type = 'tragedy'\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd51b6-4079-4c9d-8602-4b2f1af984f9",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e29e54-a3db-43ad-8c57-b437e75aaec5",
   "metadata": {},
   "source": [
    "`GROUP BY` aggregates the data, such as counting, adding, and finding the minimum or maximum, in groups of rows with the same value of some variable (`type` in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ca32a-bd26-4ae6-b0eb-99e838c197ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT type, COUNT(*) AS number, SUM(characters), MIN(year_low), MAX(year_high) FROM works GROUP BY type\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e9da37-eca9-428f-b672-d82b7f6b5cbb",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ba1b3-5510-40be-b1e7-cdd718c5ec73",
   "metadata": {},
   "source": [
    "## Fie Upon Thee, Autocomplete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d71a61-9054-4a43-adf7-3c953576aadf",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b50fd-6f3c-485d-81fc-c74e2a66013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "for i in range(len(tokens) - 1):\n",
    "    ngrams.append([tokens[i], tokens[i + 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671de6b-5d5a-4193-8f6d-5fa211399017",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"CREATE TABLE ngrams2(word1 TEXT, word2 TEXT)\")\n",
    "db.executemany(\"INSERT INTO ngrams2 VALUES(?, ?)\", ngrams)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1222964-eca0-4e9f-b6a3-ff66d0fe07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT * from ngrams2\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae0418-c186-4d89-9a68-2c9ad50f475b",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdae570-2d5b-4baf-acab-b78e6828ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"CREATE TABLE ngrams2_count AS SELECT word1, word2, COUNT(*) AS count FROM ngrams2 GROUP BY word1, word2 ORDER BY -count\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8927043-062b-4e83-9c32-00e7830e9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT * from ngrams2_count\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef8a7d2-f313-47fd-a84e-bc1f5fdb6673",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610974b-2bab-4d48-bb99-784f14185341",
   "metadata": {},
   "source": [
    "Querying the database table as an autocomplete engine: what's the most likely token after `prompt`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6a0ab-ca40-4617-bdd1-c05058fdc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be5c38-0515-43d3-86f6-89732b4a3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions = db.execute(\"SELECT word1, word2, count FROM ngrams2_count WHERE word1=?\", prompt).fetchmany(30)\n",
    "completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbcf3c1-2d87-4ad8-9683-7e66ebbfef1d",
   "metadata": {},
   "source": [
    "Okay, these are some good matches, but we want more context for larger prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af058014-e77f-4c29-9526-d344d9121639",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883aa984-fa86-4a70-bea2-0e2e04fdbbf6",
   "metadata": {},
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea28c2-8819-4de9-a596-05f881e79483",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "for i in range(len(tokens) - 2):\n",
    "    ngrams.append([tokens[i], tokens[i + 1], tokens[i + 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875c02d-047e-457f-86fb-d57d8c05527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"CREATE TABLE ngrams3(word1 TEXT, word2 TEXT, word3 TEXT)\")\n",
    "db.executemany(\"INSERT INTO ngrams3 VALUES(?, ?, ?)\", ngrams)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e43d57-5fbd-4990-81f0-81b8a52768ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"CREATE TABLE ngrams3_count AS SELECT word1, word2, word3, COUNT(*) AS count FROM ngrams3 GROUP BY word1, word2, word3 ORDER BY -count\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0bfa3-fb7f-4909-916a-9a9fc7187c96",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca88520-768e-4956-88be-9a3268957210",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"O\", \"Romeo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8dfbd4-6738-4ac8-addc-0b068f85d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions = db.execute(\"SELECT word1, word2, word3, count FROM ngrams3_count WHERE word1=? AND word2=?\", prompt).fetchmany(30)\n",
    "completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b102d86-f1f3-4222-a435-dec146b53f1b",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77834fd2-cf59-4a8b-8514-403140e9ff7f",
   "metadata": {},
   "source": [
    "If we take the first of these, where does it lead us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef5014-fe1f-412b-b625-4693c55c0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = prompt + [completions[0][2]]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5703db-98d7-4f87-89a7-9379c286865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions = db.execute(\"SELECT word1, word2, word3, count FROM ngrams3_count WHERE word1=? AND word2=?\", history[-2:]).fetchmany(30)\n",
    "completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe6c32-ff8f-4412-afec-922243d5d3bb",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8897e91d-5b88-4e6d-b35f-cf37c3326055",
   "metadata": {},
   "source": [
    "Keep doing it! Run this cell repeatedly with control-enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466c6a02-0a15-4902-b0e7-89d007daee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = history + [completions[0][2]]\n",
    "completions = db.execute(\"SELECT word1, word2, word3, count FROM ngrams3_count WHERE word1=? AND word2=?\", history[-2:]).fetchmany(30)\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54910b4f-ac25-4d36-a035-dcacdd660a61",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb06bf-d15f-4198-807c-2cd9717e4bea",
   "metadata": {},
   "source": [
    "Okay, maybe we shouldn't always take the most common completion. Maybe we should vary it up and randomly pick from the top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253fadfd-1ef7-47bd-a6c6-249ba0ff6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\"O\", \"Romeo\"]\n",
    "\n",
    "def autocomplete(history, number):\n",
    "    completions = db.execute(\"SELECT word1, word2, word3, count FROM ngrams3_count WHERE word1=? AND word2=?\", history[-2:]).fetchmany(number)\n",
    "    index = np.random.randint(0, len(completions))\n",
    "    return history + [completions[index][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a018280f-f6d7-4205-a9fb-9e809db61b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autocomplete(history, 5)\n",
    "\n",
    "for token in history:\n",
    "    if token in \",;:‚Äî.!?‚Äú‚Äù‚Äò&\":\n",
    "        prefix = \"\"\n",
    "    else:\n",
    "        prefix = \" \"\n",
    "    print(prefix + token, end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4009567-d284-479a-8c0b-cf2f10cdefa8",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4968fb-07af-4063-ab98-c433f0822354",
   "metadata": {},
   "source": [
    "What happens if we use the top 20 instead of the top 5? What if we use the top 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbb894-75b9-45aa-98a2-bfb447c00937",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb25ff-c501-481d-9b77-76e26fc24188",
   "metadata": {},
   "source": [
    "### 10-minute exercise: 4-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a3fa2-3d16-4d4f-b955-7d2557cdb5ab",
   "metadata": {},
   "source": [
    "Make it better by building a database (model) of completions that are 4 tokens long!\n",
    "\n",
    "If you need to delete a table because of a mistake, use the `DROP TABLE <name>` syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648bbca-105f-420c-843e-976fb85d38c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"DROP TABLE ngrams2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a8bfc-62ba-496b-9ce1-c9c073f536e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"DROP TABLE ngrams2_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f905c-7450-4de9-8279-b19a2ddbd7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"DROP TABLE ngrams3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c54eb-4e09-44f3-9e70-88280346e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"DROP TABLE ngrams3_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34b8689-b314-4b27-8f3b-989cb912a66d",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c12e10e-6496-4dea-a4f9-b9331f574916",
   "metadata": {},
   "source": [
    "After an `INSERT INTO`, you need to `db.commit`, and after a `SELECT`, you have to `fetchmany`, as in the examples above.\n",
    "\n",
    "**Hint:** Copy the cells of the 3-gram case and modify it to make 4-grams.\n",
    "\n",
    "Form groups of 2 or 3 and work together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c863eb4-906f-420c-b86b-afab92240724",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6861f5-84bb-432f-987a-79009c3be01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "for i in range(len(tokens) - 3):\n",
    "    ngrams.append([tokens[i], tokens[i + 1], tokens[i + 2], tokens[i + 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406dd178-5c52-42cb-bf24-6eaada852b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ngrams4 table and fill it with the ngrams data.\n",
    "print(\"???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4136a3a-1375-4d2f-96eb-2bad6217e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a GROUP BY operation to count the number of times each unique (word1, word2, word3, word4) combination is seen.\n",
    "print(\"???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddd90d-1be4-4421-b02f-3f82ae921766",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\"O\", \"Romeo\", \",\"]\n",
    "\n",
    "# Extend the autocomplete function to return (word1, word2, word3, word4) rows in which (word1, word2, word3) are the last 3 in the history.\n",
    "def autocomplete(history, number):\n",
    "\n",
    "    print(\"???\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4cc88c-3dbe-4139-8f73-2a9bda884c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autocomplete(history, 5)\n",
    "\n",
    "for token in history:\n",
    "    if token in \",;:‚Äî.!?‚Äú‚Äù‚Äò&\":\n",
    "        prefix = \"\"\n",
    "    else:\n",
    "        prefix = \" \"\n",
    "    print(prefix + token, end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa530a-e23c-48d2-9a3d-d2d0ebe64b22",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df58fc6-2fa3-4213-b81a-dfe12cc0966a",
   "metadata": {},
   "source": [
    "### Solution (do not peek!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d77114d-b137-45da-960f-82e742bceee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "for i in range(len(tokens) - 3):\n",
    "    ngrams.append([tokens[i], tokens[i + 1], tokens[i + 2], tokens[i + 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc29aaa-9796-414e-b38d-b4fc8cc4d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"CREATE TABLE ngrams4(word1 TEXT, word2 TEXT, word3 TEXT, word4 TEXT)\")\n",
    "db.executemany(\"INSERT INTO ngrams4 VALUES(?, ?, ?, ?)\", ngrams)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84625545-6c7e-47ae-9b76-c7fe22502a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"CREATE TABLE ngrams4_count AS SELECT word1, word2, word3, word4, COUNT(*) AS count FROM ngrams4 GROUP BY word1, word2, word3, word4 ORDER BY -count\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baedaf68-9bf7-4c2d-ae9d-0afa3e9aed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\"O\", \"Romeo\", \",\"]\n",
    "\n",
    "def autocomplete(history, number):\n",
    "    completions = db.execute(\"SELECT word1, word2, word3, word4, count FROM ngrams4_count WHERE word1=? AND word2=? AND word3=?\", history[-3:]).fetchmany(number)\n",
    "    index = np.random.randint(0, len(completions))\n",
    "    return history + [completions[index][3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c68bb-b456-4576-a838-7d18944b64b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autocomplete(history, 5)\n",
    "\n",
    "for token in history:\n",
    "    if token in \",;:‚Äî.!?‚Äú‚Äù‚Äò&\":\n",
    "        prefix = \"\"\n",
    "    else:\n",
    "        prefix = \" \"\n",
    "    print(prefix + token, end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d34de9-5eff-492f-8925-b807a328fa31",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a108a-d329-48c5-8fcd-7fcd8ca65f46",
   "metadata": {},
   "source": [
    "### More fun: three centuries of 5-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce367f80-e6eb-4a03-a651-de2cd412e55c",
   "metadata": {},
   "source": [
    "[Google's n-gram dataset](https://books.google.com/ngrams/) was derived from all the books Google could get their hands on (about 6% of all books ever published, according to their estimate).\n",
    "\n",
    "Their dataset of 5-grams is 35 TB (35,000 GB), but I reduced it to something small enough to carry on my laptop by changing formats and selecting only the kinds of tokens we would use.\n",
    "\n",
    "The database is accessible as a web server that runs SQL commands. I'll give you the URL once I've started the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f07236-7114-493c-8efa-5c5ad5df104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://???.???.???.???:12345\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200007bf-f793-4202-81b0-9ab5b354ecc0",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cffeaad-2b61-428d-bfd2-c16202230f96",
   "metadata": {},
   "source": [
    "Once you have the actual address, run the following cell to get an example query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aab02184-31e0-4e16-9abc-ba9f70a8e52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"margin: 50px; font-size: 20px;\"><a href=\"http://192.168.1.224:12345/19xx?w1=I&w2=am&w3=not&w4=the&top=20\">http://192.168.1.224:12345/19xx?w1=I&w2=am&w3=not&w4=the&top=20</a></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "sample_query = f\"{url}/19xx?w1=I&w2=am&w3=not&w4=the&top=20\"\n",
    "\n",
    "IPython.display.HTML(f'<div style=\"margin: 50px; font-size: 20px;\"><a href=\"{sample_query}\">{sample_query}</a></div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958601c-f571-4205-b8e6-4cfa095c7c46",
   "metadata": {},
   "source": [
    "This gets autocomplete statistics from the 1700's (options are `15xx`, `16xx`, `17xx`), the query defines the first four words as `w1`, `w2`, `w3`, `w4` and the maximum number of completions to return with `top`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a64a6-387e-481e-8168-75cd24bd9599",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5b222e-fc25-48f6-93b8-f0f17c8d9cd2",
   "metadata": {},
   "source": [
    "The SQL queries are\n",
    "\n",
    "```sql\n",
    "SELECT word1, word2, word3, word4, word5, count_17xx\n",
    "FROM ngrams5_count\n",
    "WHERE word1=? AND word2=? AND word3=? AND word4=?\n",
    "ORDER BY count_17xx DESC\n",
    "LIMIT 20\n",
    "```\n",
    "\n",
    "and it returns data as a dict that can be understood by Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4124960-e366-4ea7-ba30-5fcf7200408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77f36b96-8f05-4011-a721-d22fcc9ef9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completions': [['I', 'am', 'not', 'the', 'only', 10029],\n",
       "  ['I', 'am', 'not', 'the', 'first', 5657],\n",
       "  ['I', 'am', 'not', 'the', 'man', 5416],\n",
       "  ['I', 'am', 'not', 'the', 'one', 3452],\n",
       "  ['I', 'am', 'not', 'the', 'same', 2521],\n",
       "  ['I', 'am', 'not', 'the', 'least', 2470],\n",
       "  ['I', 'am', 'not', 'the', 'kind', 1911],\n",
       "  ['I', 'am', 'not', 'the', 'person', 1812],\n",
       "  ['I', 'am', 'not', 'the', 'sort', 1764],\n",
       "  ['I', 'am', 'not', 'the', 'less', 1260],\n",
       "  ['I', 'am', 'not', 'the', 'Christ', 1140],\n",
       "  ['I', 'am', 'not', 'the', 'author', 980],\n",
       "  ['I', 'am', 'not', 'the', 'best', 911],\n",
       "  ['I', 'am', 'not', 'the', 'body', 897],\n",
       "  ['I', 'am', 'not', 'the', 'son', 682],\n",
       "  ['I', 'am', 'not', 'the', 'type', 628],\n",
       "  ['I', 'am', 'not', 'the', 'cause', 588],\n",
       "  ['I', 'am', 'not', 'the', 'most', 571],\n",
       "  ['I', 'am', 'not', 'the', 'woman', 502],\n",
       "  ['I', 'am', 'not', 'the', 'master', 499]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url + \"/19xx\", params={\"w1\": \"I\", \"w2\": \"am\", \"w3\": \"not\", \"w4\": \"the\", \"top\": 20}).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c65e8c7-4ebd-4aa7-9de0-e302ef479b3d",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "id": "7c8301de-83e3-4d72-852d-95db8e85e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_randomly(completions, temperature):\n",
    "    index = np.random.randint(0, len(completions))\n",
    "\n",
    "    return completions[index][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "id": "5351706d-3898-4678-b669-82c94e81b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_most_likely(completions, temperature):\n",
    "    counts = np.zeros(len(completions))\n",
    "    for i, completion in enumerate(completions):\n",
    "        counts[i] = completion[-1]\n",
    "\n",
    "    index = np.argmax(counts)\n",
    "\n",
    "    return completions[index][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "id": "0445c799-430b-41a2-aa6f-039bfcc5b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@balci.pelin/llm-temperature-659d443b855a\n",
    "def choose_by_softmax(completions, temperature):\n",
    "    counts = np.zeros(len(completions))\n",
    "    for i, completion in enumerate(completions):\n",
    "        counts[i] = completion[-1]\n",
    "    weight = counts / np.sum(counts)\n",
    "\n",
    "    numerators = np.exp(weight / temperature)\n",
    "    denominator = np.sum(numerators)\n",
    "    softmax = numerators / denominator\n",
    "\n",
    "    pick_a_number = np.random.uniform(np.min(softmax) - 1e-10, np.max(softmax) - 1e-10)\n",
    "    index = np.count_nonzero(softmax > pick_a_number) - 1\n",
    "    \n",
    "    return completions[index][-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d8ebb-3fc8-4035-8758-7bb1eada1b80",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "cafbaea3-33a1-4a9e-b8d3-25d3d4ccb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocomplete(history, century, temperature):\n",
    "    params = {\n",
    "        \"w1\": history[-4],\n",
    "        \"w2\": history[-3],\n",
    "        \"w3\": history[-2],\n",
    "        \"w4\": history[-1],\n",
    "        \"top\": 100,\n",
    "    }\n",
    "    output = requests.get(f\"{url}/{century}\", params=params).json()\n",
    "    if \"completions\" not in output:\n",
    "        raise ValueError(f\"server returned: {output['error']}\")\n",
    "    if len(output[\"completions\"]) == 0:\n",
    "        return history\n",
    "    return history + [choose_by_softmax(output[\"completions\"], temperature)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf66d9-bf97-46df-b16d-3f5d3bd63bde",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "id": "2a6609f5-4b83-401b-942c-bb5638249465",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\"I\", \"am\", \"not\", \"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "id": "1b1a539a-f233-476e-be89-e7f4631878b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am not the one who is in the habit of reading the Bible. The first of these is the one at the top of the hill and was out of sight. The idea of the king as the one who was'the first man said.'I'll put a girdle round the waist. In some the whole the of county government The the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "history = autocomplete(history, \"19xx\", 0.5)\n",
    "\n",
    "no_space = [\"'\", \"-\"]\n",
    "\n",
    "previous = \"\"\n",
    "for token in history:\n",
    "    if token in \",;:‚Äî.!?‚Äú‚Äù‚Äò\\\"'&\" or any(token.startswith(x) or previous.endswith(x) for x in no_space):\n",
    "        prefix = \"\"\n",
    "    else:\n",
    "        prefix = \" \"\n",
    "    previous = token\n",
    "    print(prefix + token, end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce50126-df5c-4b02-9b94-68472a070ea0",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbce536-41ea-475e-a36f-17216733cb90",
   "metadata": {},
   "source": [
    "The results are looking better, and that's because this database has 235,619,673 5-grams instead of 1,071,353 4-grams.\n",
    "\n",
    "The model‚Äîcorrelations among words‚Äîis more detailed and therefore more representative of the measurements‚Äîwords that you'd find in books."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe91369-55c5-4585-bd55-62779ac20f8b",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe27b4-bc63-4079-990e-a47bc217834b",
   "metadata": {},
   "source": [
    "Large language models (LLMs) like ChatGPT are also autocomplete engines.\n",
    "\n",
    "LLMs have much larger databases, and they're also more sophisticated algorithms.\n",
    "\n",
    "But they fundamentally have the data-type of an autocomplete engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a898b6e2-5219-4e96-9c26-34dda42aa5a5",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5592de-f7e9-44b9-9c11-c16102858a5e",
   "metadata": {},
   "source": [
    "The following will only work for me (presenting) because I've loaded my OpenAI password into my notebook, not yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd364ed7-d0e7-4c41-989a-ae9ff7715385",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    \"https://api.openai.com/v1/completions\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "    },\n",
    "    json={\n",
    "        \"model\": \"gpt-3.5-turbo-instruct\",\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0,\n",
    "        \"echo\": True,\n",
    "        \"prompt\": \"I am not the\",\n",
    "    },\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210e670-2b37-4771-9315-6160b5b32812",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f78c7c-b6be-4b52-ab47-06c7479a9a09",
   "metadata": {},
   "source": [
    "In the past year, we've become accustomed to seeing LLMs holding a back-and-forth conversation, but that's just a particular choice of autocomplete-prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c6e868-b8ff-4164-8687-b250735ae352",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    \"https://api.openai.com/v1/completions\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "    },\n",
    "    json={\n",
    "        \"model\": \"gpt-3.5-turbo-instruct\",\n",
    "        \"max_tokens\": 30,\n",
    "        \"temperature\": 0,\n",
    "        \"echo\": True,\n",
    "        \"prompt\": \"\"\"Human: Tell me a joke.\n",
    "AI: Why did the chicken cross the road?\n",
    "Human: That's not a very good joke.\n",
    "AI: \"\"\",\n",
    "    },\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2ded5-9699-4d00-b6f9-2d7fcecb85ee",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f028fd7-723e-45f4-89e4-3c5187c04bad",
   "metadata": {},
   "source": [
    "When you send a message to ChatGPT, you're actually sending the whole dialog to an OpenAI server that likely hasn't seen it before. (The previous messages were handled by other, random, servers.) This server looks at the dialog so far and autocompletes it one message further.\n",
    "\n",
    "Here, we have it autocomplete in the role of the human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a1bca-584b-44a5-a234-b89431595157",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    \"https://api.openai.com/v1/completions\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "    },\n",
    "    json={\n",
    "        \"model\": \"gpt-3.5-turbo-instruct\",\n",
    "        \"max_tokens\": 30,\n",
    "        \"temperature\": 0,\n",
    "        \"echo\": True,\n",
    "        \"prompt\": \"\"\"Human: Tell me a joke.\n",
    "AI: Why did the chicken cross the road?\n",
    "Human: That's not a very good joke.\n",
    "AI: Okay, how about this one: Why did the tomato turn red? Because it saw the salad dressing!\n",
    "Human: \"\"\",\n",
    "    },\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e416a5-5620-4c74-9404-b18d75a3e6d3",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71061535-e97a-45ea-8546-30fc461c5b58",
   "metadata": {},
   "source": [
    "Apparently, \"Haha, that's better. Do you have any more jokes?\" is a likely continuation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
