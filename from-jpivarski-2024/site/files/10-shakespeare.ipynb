{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c00b45-c571-4438-be9b-60f80e8be054",
   "metadata": {},
   "source": [
    "# Fie Upon Thee, Autocorrect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54dee98-c213-4197-b2eb-8184fa70d7ea",
   "metadata": {},
   "source": [
    "<img src=\"img/shakespeare.jpg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd8904-0e5d-41bc-98b8-6d56c14eb7db",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d2eaf8-e86f-4f3d-8c15-38b5597ccfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30075a76-6d43-40da-adcd-cb14acc3f3ce",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046cc6b9-7f8f-4243-b018-5c6a34e81957",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset: the complete works of Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9f28d-7ca0-4a42-a8e2-3b7780f914f8",
   "metadata": {},
   "source": [
    "This used to be a big dataset, used to illustrate large storage devices, like in [this definition of CD-ROM](https://vintageapple.org/apple_ii/pdf/Apple_IIGS__Ownwers_Guide_1986.pdf) from 1986:\n",
    "\n",
    "<img src=\"img/shakespeare-a-big-dataset.png\" width=\"600\">\n",
    "\n",
    "Now it's small enough to easily load in JupyterLite but is still big enough to be interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e5f10-bc12-46fc-ba20-de8f8b8a329e",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8767d-18fb-4b0c-907e-eab6720f13d3",
   "metadata": {},
   "source": [
    "This file comes from Project Gutenberg, [ebook #100](https://www.gutenberg.org/ebooks/100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e692285-26fe-4e53-9cbd-95ea094627c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/shakespeare.txt\") as file:\n",
    "    corpus = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2cd8d-2291-4550-b468-56bc01368edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59238d63-7e71-4976-b412-4df3c5b5f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus) / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79440957-226f-4d21-add6-ee5ec85ead69",
   "metadata": {},
   "source": [
    "5.36 MB (a whole laser disk, apparently)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69243038-8b18-4bf1-95cf-10846f219be9",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41439cb7-f5f1-466c-9006-966543672fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[100000:101000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0ba72c-2114-44d3-a833-89e824cf1f61",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626cb458-2dfd-4c5d-8e01-6fe22e7439a5",
   "metadata": {},
   "source": [
    "What distinct characters does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc451885-2555-4260-a0a2-6083e6f9d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84875aef-4b40-4c1d-aead-e04e847fcca3",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117f359c-752f-4d3a-96e6-61790753fd59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## How often is \"t\" followed by \"h\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02184a24-fc46-4dbe-94b1-14484447814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_character = []\n",
    "next_character = []\n",
    "for i in range(len(corpus) - 1):\n",
    "    first_character.append(corpus[i])\n",
    "    next_character.append(corpus[i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d05d4c-6609-4ac0-9227-22ef581be17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_character[100414:100439]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ebf782-ccb3-4d5f-83ca-23ce4ece03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_character[100414:100439]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5805a-68d8-4d41-960a-4a1847dbd81c",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563e70f-ec58-44fc-872a-5ff978a71df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.crosstab(first_character, next_character, rownames=[\"first\"], colnames=[\"next\"])\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4603180-edd3-4c78-b211-c54abe9c9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_columns = pairs[pairs.sum(axis=0).sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d055a7-b3b5-4784-9646-dca076591d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_both = sorted_by_columns.loc[sorted_by_columns.sum(axis=1).sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250301d0-4d0a-49da-b6aa-dada4039356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7268d0-8d10-476e-bfac-c7b1e8650951",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "matrix = ax.matshow(sorted_by_both.values)\n",
    "\n",
    "ax.set_xticks(range(len(sorted_by_both.index)), sorted_by_both.index)\n",
    "ax.set_yticks(range(len(sorted_by_both.columns)), sorted_by_both.columns)\n",
    "\n",
    "ax.set_xlim(-0.5, 25.5)\n",
    "ax.set_ylim(-0.5, 25.5)\n",
    "ax.set_ylabel(\"first character\")\n",
    "ax.set_xlabel(\"next character\")\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2045102-6a91-4d9a-8fcb-d57466656506",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36858464-3be9-41e0-acf2-9f7e392e9463",
   "metadata": {},
   "source": [
    "The bright spots are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f193d-729b-445f-8b1b-181dd7afe69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.loc[\"e\", \" \"]   # e followed by space (at the end of a word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5903a8-3e8a-416a-b0cd-9e496946198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.loc[\" \", \"t\"]   # space followed by t (at the beginning of a word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb444e91-2654-4648-9132-a8a97a2f00b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.loc[\"t\", \"h\"]   # t followed by h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82fa19-da0a-4f82-b7c6-5f3312dedade",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.loc[\".\", \" \"]   # period followed by space (at the end of a sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b83f40-5435-46aa-94f0-27d5341a44ac",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650cac4-b72d-4fc4-9501-30839d99505d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## From sequence of characters to sequence of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea2662-9030-4454-9de5-ff6d7f90c12c",
   "metadata": {},
   "source": [
    "We could build a per-letter autocomplete algorithm that would see \"t\" and suggest \"h\", but it wouldn't produce interesting text.\n",
    "\n",
    "It gets more interesting if we do this at the word level.\n",
    "\n",
    "The first step of **parsing**, an analysis of human-readable text, is **tokenizing** the input: turning raw characters into **tokens**.\n",
    "\n",
    "Why \"tokens\" and not \"words\"? Some of our tokens will be punctuation marks, so that when your autocomplete algorithm sees `hark` it can suggest a token like `!`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c15f17c-4e07-4a0e-8668-4bc683d34404",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03dfc1-ed92-430c-b384-c3a5f967d654",
   "metadata": {},
   "source": [
    "**Regular expressions** or **regex** is a mini-language for recognizing strings and parts of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b332a30-9e4c-4fec-b5f8-71d565073390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c72543-5ca1-4442-af57-e5638ea68869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#               \"’\" for \"thou be’st\" and \"-\" for \"a-foot\"       multi-digit number as a token\n",
    "#                olden-time (and French) letters     🡓    \"&c.\"  🡓  \"+\" matches sequences of at least 1 character\n",
    "#        capital and lowercase letters       🡓       🡓     🡓     🡓  🡓   match any single character that is not (^) a space\n",
    "#                                  🡓         🡓       🡓     🡓     🡓  🡓   🡓\n",
    "recognize_token = re.compile(\"([A-Za-zÀÆÇÉàâæçèéêëîœ’-]+|&c\\.|[0-9]+|[^ ])\")\n",
    "#                            🡑                          🡑    🡑      🡑\n",
    "#              \"(\" and \")\" form a group               \"|\" means \"or\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a3774-ef34-4793-960a-636149cca0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognize_token.findall(\"Dost thou be’st a tokenized sen-tënce, &c.?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968d6b1-ea41-42d0-af3a-eea5897d58cf",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94cc2a7-11a7-4d1b-84ed-eb7aadbf0c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in recognize_token.finditer(corpus):\n",
    "    token = match.group(0)\n",
    "    print(repr(token))\n",
    "    if token == \".\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a99f24-39dc-4924-acae-b4a25d633777",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a0ae9-7163-4ea9-a2a2-562decb6413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for match in recognize_token.finditer(corpus):\n",
    "    tokens.append(match.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335e092-e7b4-49be-8a0d-99b63f799a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77627f43-4869-4e49-9503-8ef99f936841",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88638040-b7f2-4987-ab0a-0941facdba85",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac71590-b514-44d6-89ac-74b88309fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da30d44-72ce-4754-9471-a72b57bfc869",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26e7b0-d199-439f-947a-07ed1fa653f8",
   "metadata": {},
   "source": [
    "Instead of a 103×103 table of \"first\", \"next\" pairs, this would be a 36775×36775 table. Too big!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bcdc5-531d-464e-b9e8-e15149f96d62",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537db7ff-6fa2-493c-bc51-bc8e1c1dc52e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## SQL, the language of table manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2311070e-d321-4297-a6d5-d2d42c771bc4",
   "metadata": {},
   "source": [
    "Just as **regular expression** is a mini-language that we can call from Python to handle strings, **SQL** is a language to deal with tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acab08f-6311-44b4-ab17-7c884a248157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc085d0-2804-47f0-9c08-eea3099a6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(\":memory:\")\n",
    "db.execute(\"CREATE TABLE works(title TEXT, type TEXT, characters INTEGER, year_low INTEGER, year_high INTEGER)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece55759-ceb4-41d4-850a-d441feda868a",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ebf05-0c8e-4602-9252-27a45fdee0b7",
   "metadata": {},
   "source": [
    "Some data to feed into the table, in Python format (lists, strings, and numbers).\n",
    "\n",
    "_(Don't assume these numbers are correct; I got them from ChatGPT.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb6f63-b413-46ad-9d6b-eee7b77fe1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_python = [\n",
    "    [\"The Sonnets\", \"poetry\", None, 1609, 1609],\n",
    "    [\"All’s Well that Ends Well\", \"comedy\", 23, 1604, 1605],\n",
    "    [\"The Tragedy of Antony and Cleopatra\", \"tragedy\", 42, 1606, 1606],\n",
    "    [\"As You Like It\", \"comedy\", 27, 1599, 1600],\n",
    "    [\"The Comedy of Errors\", \"comedy\", 18, 1594, 1594],\n",
    "    [\"The Tragedy of Coriolanus\", \"tragedy\", 30, 1608, 1608],\n",
    "    [\"Cymbeline\", \"mixed\", 20, 1609, 1610],\n",
    "    [\"The Tragedy of Hamlet, Prince of Denmark\", \"tragedy\", 30, 1599, 1601],\n",
    "    [\"The First Part of King Henry the Fourth\", \"history\", 25, 1596, 1597],\n",
    "    [\"The Second Part of King Henry the Fourth\", \"history\", 25, 1597, 1598],\n",
    "    [\"The Life of King Henry the Fifth\", \"history\", 30, 1599, 1599],\n",
    "    [\"The First Part of Henry the Sixth\", \"history\", 40, 1590, 1592],\n",
    "    [\"The Second Part of King Henry the Sixth\", \"history\", 30, 1590, 1591],\n",
    "    [\"The Third Part of King Henry the Sixth\", \"history\", 30, 1591, 1591],\n",
    "    [\"King Henry the Eighth\", \"history\", 30, 1612, 1613],\n",
    "    [\"The Life and Death of King John\", \"history\", 20, 1596, 1596],\n",
    "    [\"The Tragedy of Julius Caesar\", \"tragedy\", 40, 1599, 1599],\n",
    "    [\"The Tragedy of King Lear\", \"tragedy\", 20, 1605, 1606],\n",
    "    [\"Love’s Labour’s Lost\", \"comedy\", 23, 1594, 1595],\n",
    "    [\"The Tragedy of Macbeth\", \"tragedy\", 20, 1606, 1606],\n",
    "    [\"Measure for Measure\", \"comedy\", 20, 1603, 1604],\n",
    "    [\"The Merchant of Venice\", \"comedy\", 22, 1596, 1597],\n",
    "    [\"The Merry Wives of Windsor\", \"comedy\", 24, 1597, 1597],\n",
    "    [\"A Midsummer Night’s Dream\", \"comedy\", 21, 1595, 1596],\n",
    "    [\"Much Ado About Nothing\", \"comedy\", 23, 1598, 1599],\n",
    "    [\"The Tragedy of Othello, the Moor of Venice\", \"tragedy\", 21, 1603, 1604],\n",
    "    [\"Pericles, Prince of Tyre\", \"late romance\", 20, 1607, 1608],\n",
    "    [\"King Richard the Second\", \"history\", 20, 1595, 1595],\n",
    "    [\"King Richard the Third\", \"history\", 30, 1592, 1593],\n",
    "    [\"The Tragedy of Romeo and Juliet\", \"tragedy\", 20, 1595, 1595],\n",
    "    [\"The Taming of the Shrew\", \"comedy\", 16, 1590, 1592],\n",
    "    [\"The Tempest\", \"late romance\", 12, 1610, 1611],\n",
    "    [\"The Life of Timon of Athens\", \"tragedy\", 20, 1605, 1606],\n",
    "    [\"The Tragedy of Titus Andronicus\", \"tragedy\", 25, 1591, 1592],\n",
    "    [\"Troilus and Cressida\", \"mixed\", 30, 1601, 1602],\n",
    "    [\"Twelfth Night; or, What You Will\", \"comedy\", 18, 1601, 1602],\n",
    "    [\"The Two Gentlemen of Verona\", \"comedy\", 20, 1589, 1593],\n",
    "    [\"The Two Noble Kinsmen\", \"comedy\", 20, 1613, 1614],\n",
    "    [\"The Winter’s Tale\", \"comedy\", 21, 1609, 1611],\n",
    "    [\"A Lover’s Complaint\", \"poetry\", None, 1609, 1609],\n",
    "    [\"The Passionate Pilgrim\", \"poetry\", None, 1599, 1599],\n",
    "    [\"The Phoenix and the Turtle\", \"poetry\", None, 1601, 1601],\n",
    "    [\"The Rape of Lucrece\", \"poetry\", 2, 1594, 1594],\n",
    "    [\"Venus and Adonis\", \"poetry\", 2, 1593, 1593],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf5f4f-6520-440c-8dd8-e8c6157a6774",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf176f46-45bf-4fc3-b5de-bb9986aaee4b",
   "metadata": {},
   "source": [
    "This `INSERT INTO` SQL command has five `?`s that get filled with each row of the data from Python.\n",
    "\n",
    "After preparing the command, `db.commit()` tells the database engine to run it (fast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc7f00f-6b04-4a12-884c-124e69b0733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.executemany(\"INSERT INTO works VALUES(?, ?, ?, ?, ?)\", data_in_python)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b4554-621d-41b6-9357-bb5f4107d8f6",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa97843-3aca-42e3-9654-71b28ce77a35",
   "metadata": {},
   "source": [
    "Although SQL will be more computationally efficient for what we want to do, it doesn't have a convenient way to print out and look at a table, so we'll dump it into Pandas when we want to view it.\n",
    "\n",
    "`SELECT` means select columns (not rows), and `*` means \"everything\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a30ef5-7ddc-4958-a05d-8934291e74d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT * FROM works\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c514d-1aa8-447c-8c91-676a60e499b3",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ca1f5-6099-435d-98e7-bdfe75b29e69",
   "metadata": {},
   "source": [
    "SQL has mathematical manipulations, such as `year_uncertainty` = `year_high - year_low`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0787c-c2a4-40fd-a56d-6eb204283b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT title, year_high - year_low AS year_uncertainty FROM works\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52012a34-7b9b-4cf0-a6f8-6e37ade1917e",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ed47b-c6a7-490d-b09e-954a7bbf4441",
   "metadata": {},
   "source": [
    "`WHERE` selects rows (not columns) by value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72974e1-68a7-4f8a-846f-46a4c899e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT * FROM works WHERE type = 'tragedy'\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd51b6-4079-4c9d-8602-4b2f1af984f9",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e29e54-a3db-43ad-8c57-b437e75aaec5",
   "metadata": {},
   "source": [
    "`GROUP BY` aggregates the data, such as counting, adding, and finding the minimum or maximum, in groups of rows with the same value of some variable (`type` in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ca32a-bd26-4ae6-b0eb-99e838c197ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT type, COUNT(*) AS number, SUM(characters), MIN(year_low), MAX(year_high) FROM works GROUP BY type\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e9da37-eca9-428f-b672-d82b7f6b5cbb",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ba1b3-5510-40be-b1e7-cdd718c5ec73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fie Upon Thee, Autocomplete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d71a61-9054-4a43-adf7-3c953576aadf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b50fd-6f3c-485d-81fc-c74e2a66013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "for i in range(len(tokens) - 1):\n",
    "    ngrams.append([tokens[i], tokens[i + 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671de6b-5d5a-4193-8f6d-5fa211399017",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"CREATE TABLE ngrams2(word1 TEXT, word2 TEXT)\")\n",
    "db.executemany(\"INSERT INTO ngrams2 VALUES(?, ?)\", ngrams)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1222964-eca0-4e9f-b6a3-ff66d0fe07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT * from ngrams2\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae0418-c186-4d89-9a68-2c9ad50f475b",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdae570-2d5b-4baf-acab-b78e6828ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"CREATE TABLE ngrams2_count AS SELECT word1, word2, COUNT(*) AS count FROM ngrams2 GROUP BY word1, word2 ORDER BY -count\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8927043-062b-4e83-9c32-00e7830e9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT * from ngrams2_count\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef8a7d2-f313-47fd-a84e-bc1f5fdb6673",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610974b-2bab-4d48-bb99-784f14185341",
   "metadata": {},
   "source": [
    "Querying the database table as an autocomplete engine: what's the most likely token after `prompt`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6a0ab-ca40-4617-bdd1-c05058fdc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be5c38-0515-43d3-86f6-89732b4a3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions = db.execute(\"SELECT word1, word2, count FROM ngrams2_count WHERE word1=?\", prompt).fetchmany(30)\n",
    "completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbcf3c1-2d87-4ad8-9683-7e66ebbfef1d",
   "metadata": {},
   "source": [
    "Okay, these are some good matches, but we want more context for larger prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af058014-e77f-4c29-9526-d344d9121639",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883aa984-fa86-4a70-bea2-0e2e04fdbbf6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea28c2-8819-4de9-a596-05f881e79483",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "for i in range(len(tokens) - 2):\n",
    "    ngrams.append([tokens[i], tokens[i + 1], tokens[i + 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875c02d-047e-457f-86fb-d57d8c05527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"CREATE TABLE ngrams3(word1 TEXT, word2 TEXT, word3 TEXT)\")\n",
    "db.executemany(\"INSERT INTO ngrams3 VALUES(?, ?, ?)\", ngrams)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e43d57-5fbd-4990-81f0-81b8a52768ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"CREATE TABLE ngrams3_count AS SELECT word1, word2, word3, COUNT(*) AS count FROM ngrams3 GROUP BY word1, word2, word3 ORDER BY -count\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0bfa3-fb7f-4909-916a-9a9fc7187c96",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca88520-768e-4956-88be-9a3268957210",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"O\", \"Romeo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8dfbd4-6738-4ac8-addc-0b068f85d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions = db.execute(\"SELECT word1, word2, word3, count FROM ngrams3_count WHERE word1=? AND word2=?\", prompt).fetchmany(30)\n",
    "completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b102d86-f1f3-4222-a435-dec146b53f1b",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77834fd2-cf59-4a8b-8514-403140e9ff7f",
   "metadata": {},
   "source": [
    "If we take the first of these, where does it lead us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef5014-fe1f-412b-b625-4693c55c0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = prompt + [completions[0][2]]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5703db-98d7-4f87-89a7-9379c286865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions = db.execute(\"SELECT word1, word2, word3, count FROM ngrams3_count WHERE word1=? AND word2=?\", history[-2:]).fetchmany(30)\n",
    "completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe6c32-ff8f-4412-afec-922243d5d3bb",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8897e91d-5b88-4e6d-b35f-cf37c3326055",
   "metadata": {},
   "source": [
    "Keep doing it! Run this cell repeatedly with control-enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466c6a02-0a15-4902-b0e7-89d007daee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = history + [completions[0][2]]\n",
    "completions = db.execute(\"SELECT word1, word2, word3, count FROM ngrams3_count WHERE word1=? AND word2=?\", history[-2:]).fetchmany(30)\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54910b4f-ac25-4d36-a035-dcacdd660a61",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb06bf-d15f-4198-807c-2cd9717e4bea",
   "metadata": {},
   "source": [
    "Okay, maybe we shouldn't always take the most common completion. Maybe we should vary it up and randomly pick from the top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253fadfd-1ef7-47bd-a6c6-249ba0ff6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\"O\", \"Romeo\"]\n",
    "\n",
    "def autocomplete(history, number):\n",
    "    completions = db.execute(\"SELECT word1, word2, word3, count FROM ngrams3_count WHERE word1=? AND word2=?\", history[-2:]).fetchmany(number)\n",
    "    index = np.random.randint(0, len(completions))\n",
    "    return history + [completions[index][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a018280f-f6d7-4205-a9fb-9e809db61b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autocomplete(history, 5)\n",
    "\n",
    "for token in history:\n",
    "    if token in \",;:—.!?“”‘&\":\n",
    "        prefix = \"\"\n",
    "    else:\n",
    "        prefix = \" \"\n",
    "    print(prefix + token, end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4009567-d284-479a-8c0b-cf2f10cdefa8",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4968fb-07af-4063-ab98-c433f0822354",
   "metadata": {},
   "source": [
    "What happens if we use the top 20 instead of the top 5? What if we use the top 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbb894-75b9-45aa-98a2-bfb447c00937",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb25ff-c501-481d-9b77-76e26fc24188",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 10-minute exercise: 4-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a3fa2-3d16-4d4f-b955-7d2557cdb5ab",
   "metadata": {},
   "source": [
    "Make it better by building a database (model) of completions that are 4 tokens long!\n",
    "\n",
    "If you need to delete a table because of a mistake, use the `DROP TABLE <name>` syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648bbca-105f-420c-843e-976fb85d38c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"DROP TABLE ngrams2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a8bfc-62ba-496b-9ce1-c9c073f536e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"DROP TABLE ngrams2_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f905c-7450-4de9-8279-b19a2ddbd7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"DROP TABLE ngrams3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c54eb-4e09-44f3-9e70-88280346e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"DROP TABLE ngrams3_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34b8689-b314-4b27-8f3b-989cb912a66d",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c12e10e-6496-4dea-a4f9-b9331f574916",
   "metadata": {},
   "source": [
    "After an `INSERT INTO`, you need to `db.commit`, and after a `SELECT`, you have to `fetchmany`, as in the examples above.\n",
    "\n",
    "**Hint:** Copy the cells of the 3-gram case and modify it to make 4-grams.\n",
    "\n",
    "Form groups of 2 or 3 and work together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c863eb4-906f-420c-b86b-afab92240724",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6861f5-84bb-432f-987a-79009c3be01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "for i in range(len(tokens) - 3):\n",
    "    ngrams.append([tokens[i], tokens[i + 1], tokens[i + 2], tokens[i + 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406dd178-5c52-42cb-bf24-6eaada852b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ngrams4 table and fill it with the ngrams data.\n",
    "print(\"???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4136a3a-1375-4d2f-96eb-2bad6217e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a GROUP BY operation to count the number of times each unique (word1, word2, word3, word4) combination is seen.\n",
    "print(\"???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddd90d-1be4-4421-b02f-3f82ae921766",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\"O\", \"Romeo\", \",\"]\n",
    "\n",
    "# Extend the autocomplete function to return (word1, word2, word3, word4) rows in which (word1, word2, word3) are the last 3 in the history.\n",
    "def autocomplete(history, number):\n",
    "\n",
    "    print(\"???\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4cc88c-3dbe-4139-8f73-2a9bda884c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autocomplete(history, 5)\n",
    "\n",
    "for token in history:\n",
    "    if token in \",;:—.!?“”‘&\":\n",
    "        prefix = \"\"\n",
    "    else:\n",
    "        prefix = \" \"\n",
    "    print(prefix + token, end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa530a-e23c-48d2-9a3d-d2d0ebe64b22",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df58fc6-2fa3-4213-b81a-dfe12cc0966a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution (do not peek!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d77114d-b137-45da-960f-82e742bceee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "for i in range(len(tokens) - 3):\n",
    "    ngrams.append([tokens[i], tokens[i + 1], tokens[i + 2], tokens[i + 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc29aaa-9796-414e-b38d-b4fc8cc4d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"CREATE TABLE ngrams4(word1 TEXT, word2 TEXT, word3 TEXT, word4 TEXT)\")\n",
    "db.executemany(\"INSERT INTO ngrams4 VALUES(?, ?, ?, ?)\", ngrams)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84625545-6c7e-47ae-9b76-c7fe22502a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"CREATE TABLE ngrams4_count AS SELECT word1, word2, word3, word4, COUNT(*) AS count FROM ngrams4 GROUP BY word1, word2, word3, word4 ORDER BY -count\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baedaf68-9bf7-4c2d-ae9d-0afa3e9aed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\"O\", \"Romeo\", \",\"]\n",
    "\n",
    "def autocomplete(history, number):\n",
    "    completions = db.execute(\"SELECT word1, word2, word3, word4, count FROM ngrams4_count WHERE word1=? AND word2=? AND word3=?\", history[-3:]).fetchmany(number)\n",
    "    index = np.random.randint(0, len(completions))\n",
    "    return history + [completions[index][3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c68bb-b456-4576-a838-7d18944b64b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autocomplete(history, 5)\n",
    "\n",
    "for token in history:\n",
    "    if token in \",;:—.!?“”‘&\":\n",
    "        prefix = \"\"\n",
    "    else:\n",
    "        prefix = \" \"\n",
    "    print(prefix + token, end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d34de9-5eff-492f-8925-b807a328fa31",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a108a-d329-48c5-8fcd-7fcd8ca65f46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## More fun: centuries of 5-grams (demo only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef418b-7a1c-483a-bc55-439dffdd76bd",
   "metadata": {},
   "source": [
    "[Google's n-gram dataset](https://books.google.com/ngrams/) was derived from all the books Google could get their hands on (about 6% of all books ever published, according to their estimate).\n",
    "\n",
    "I used it to make a database of 368,799,605 5-grams (compare to 1,071,353 4-grams in the Shakespeare database)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d00cb-e119-45c0-bbf1-3f57aa5df6b6",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd885161-3707-42a9-b4f1-2c74e2aecbeb",
   "metadata": {},
   "source": [
    "This demo will only work on my computer, as I present it, because the database is on my laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ea1fe-0356-42ed-b4fb-6b23f180dd57",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfeef2c-0739-4cad-8080-f85ed3772189",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_db = sqlite3.connect(\"/Users/jpivarski/big.db\")\n",
    "\n",
    "def next_words(words, count_per_century, top_n):\n",
    "    # try to match 4 words and return the 5th\n",
    "    completions = big_db.execute(\n",
    "        f\"\"\"\n",
    "SELECT word5, {count_per_century}\n",
    "FROM ngrams5_count\n",
    "WHERE {count_per_century}>0 AND word1=? AND word2=? AND word3=? AND word4=?\n",
    "ORDER BY {count_per_century} DESC\n",
    "LIMIT {top_n}\n",
    "\"\"\",\n",
    "        words,\n",
    "    ).fetchmany(top_n)\n",
    "\n",
    "    if len(completions) == 0:\n",
    "        # try to match 3 words and return the 4th and 5th\n",
    "        completions = big_db.execute(\n",
    "        f\"\"\"\n",
    "SELECT word4, word5, {count_per_century}\n",
    "FROM ngrams5_count\n",
    "WHERE {count_per_century}>0 AND word1=? AND word2=? AND word3=?\n",
    "ORDER BY {count_per_century} DESC\n",
    "LIMIT {top_n}\n",
    "\"\"\",\n",
    "            words[1:],\n",
    "        ).fetchmany(top_n)\n",
    "\n",
    "    if len(completions) == 0:\n",
    "        # try to match 2 words and return the 3rd, 4th, and 5th\n",
    "        completions = big_db.execute(\n",
    "        f\"\"\"\n",
    "SELECT word3, word4, word5, {count_per_century}\n",
    "FROM ngrams5_count\n",
    "WHERE {count_per_century}>0 AND word1=? AND word2=?\n",
    "ORDER BY {count_per_century} DESC\n",
    "LIMIT {top_n}\n",
    "\"\"\",\n",
    "            words[2:],\n",
    "        ).fetchmany(top_n)\n",
    "\n",
    "    if len(completions) == 0:\n",
    "        # try to match 1 word and return the 2nd, 3rd, 4th, and 5th\n",
    "        completions = big_db.execute(\n",
    "        f\"\"\"\n",
    "SELECT word2, word3, word4, word5, {count_per_century}\n",
    "FROM ngrams5_count\n",
    "WHERE {count_per_century}>0 AND word1=?\n",
    "ORDER BY {count_per_century} DESC\n",
    "LIMIT {top_n}\n",
    "\"\"\",\n",
    "            words[3:],\n",
    "        ).fetchmany(top_n)\n",
    "\n",
    "    return completions\n",
    "\n",
    "def choose_randomly(completions): \n",
    "    index = np.random.randint(0, len(completions))\n",
    "    return list(completions[index][:-1])\n",
    "\n",
    "def autocomplete(history, count_per_century, top_n):\n",
    "    if history[-1] == \"(No more completions!)\":\n",
    "        return history\n",
    "\n",
    "    completions = next_words(history[-4:], count_per_century, top_n)\n",
    "\n",
    "    if len(completions) == 0:\n",
    "        return history + [\"(No more completions!)\"]\n",
    "\n",
    "    return history + choose_randomly(completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3306466e-5a7c-41aa-8b01-119776343c3c",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff66648-1f6f-4f24-8194-62ac3b338699",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\"I\", \"am\", \"not\", \"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537334d2-91ab-4f2b-b011-89db6841c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autocomplete(history, \"count_19xx\", 10)\n",
    "\n",
    "# Print it out nicely!\n",
    "previous = \"\"\n",
    "length = 0\n",
    "no_left_space = [\"'\", \"-\", \"–\", \"—\", \",\", \":\", \";\", \".\", \"?\", \"!\", \")\"]\n",
    "no_right_space = [\"'\", \"-\", \"–\", \"—\", \"$\", \"(\"]\n",
    "for token in history:\n",
    "    length += len(token)\n",
    "    if previous == \"\" or any(token.startswith(x) for x in no_left_space) or any(previous.endswith(x) for x in no_right_space):\n",
    "        prefix = \"\"\n",
    "    else:\n",
    "        if length < 80:\n",
    "            prefix = \" \"\n",
    "        else:\n",
    "            prefix = \"\\n\"\n",
    "            length = 0\n",
    "    previous = token\n",
    "    print(prefix + token, end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d9e0b-6c08-4de1-8344-c1cc5218f163",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52060163-6ac0-45a6-8bea-5bcdd0149fb1",
   "metadata": {},
   "source": [
    "It doesn't make sense overall—it's like someone rambling without thinking because its **context window** is only 5 tokens long.\n",
    "\n",
    "Correlations between any tokens that are more than 5 tokens apart are very small, and they get more uncorrelated the farther they are apart.\n",
    "\n",
    "It doesn't look like the lucid text that comes from Large Language Models (LLMs) such as ChatGPT, but LLMs are also autocomplete engines.\n",
    "\n",
    "LLMs have much larger databases, much larger context windows, and more sophisticated algorithms for predicting the next token, but they are autocomplete engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce50126-df5c-4b02-9b94-68472a070ea0",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5592de-f7e9-44b9-9c11-c16102858a5e",
   "metadata": {},
   "source": [
    "This is OpenAI's direct interface to ChatGPT.\n",
    "\n",
    "`requests` is a Python library. `post` sends the `json` to a URL like `https://api.openai.com/v1/completions`, and the data that comes back is a dict, rather than a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f72ab3-cc06-4222-8693-0c827b9cdc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd364ed7-d0e7-4c41-989a-ae9ff7715385",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    \"https://api.openai.com/v1/completions\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "    },\n",
    "    json={\n",
    "        \"model\": \"gpt-3.5-turbo-instruct\",\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0,\n",
    "        \"logprobs\": None,\n",
    "        \"prompt\": \"I am not the\",   # the text that autocomplete will finish\n",
    "    },\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210e670-2b37-4771-9315-6160b5b32812",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0bd0fb-9859-46c6-9fb5-ed435fca87f9",
   "metadata": {},
   "source": [
    "The **temperature** parameter controls how random the output tokens can be, like our `top_n`.\n",
    "\n",
    "With `\"temperature\": 0`, the LLM will always return the same output.\n",
    "\n",
    "Values like `\"temperature\": 0.7` give nice results.\n",
    "\n",
    "The maximum, `\"temperature\": 2`, generates garbage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a2158-4e98-4ed7-98cb-fb199713e4d5",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f78c7c-b6be-4b52-ab47-06c7479a9a09",
   "metadata": {},
   "source": [
    "In the past year, we've become accustomed to seeing LLMs holding a back-and-forth conversation, but that's just a particular choice of autocomplete-prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c6e868-b8ff-4164-8687-b250735ae352",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    \"https://api.openai.com/v1/completions\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "    },\n",
    "    json={\n",
    "        \"model\": \"gpt-3.5-turbo-instruct\",\n",
    "        \"max_tokens\": 30,\n",
    "        \"temperature\": 0,\n",
    "        \"echo\": True,\n",
    "        \"prompt\": \"\"\"Human: Tell me a joke.\n",
    "AI: Why did the chicken cross the road?\n",
    "Human: That's not a very good joke.\n",
    "AI: \"\"\",   # the text that autocomplete will finish\n",
    "    },\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2ded5-9699-4d00-b6f9-2d7fcecb85ee",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f028fd7-723e-45f4-89e4-3c5187c04bad",
   "metadata": {},
   "source": [
    "When you send a message to ChatGPT, you're actually sending the whole dialog to an OpenAI server that likely hasn't seen it before. (The previous messages were handled by other, random, servers.) This server looks at the dialog so far and autocompletes it one message further.\n",
    "\n",
    "Here, we have it fill in text for the person named \"human\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a1bca-584b-44a5-a234-b89431595157",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    \"https://api.openai.com/v1/completions\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "    },\n",
    "    json={\n",
    "        \"model\": \"gpt-3.5-turbo-instruct\",\n",
    "        \"max_tokens\": 30,\n",
    "        \"temperature\": 0,\n",
    "        \"echo\": True,\n",
    "        \"prompt\": \"\"\"Human: Tell me a joke.\n",
    "AI: Why did the chicken cross the road?\n",
    "Human: That's not a very good joke.\n",
    "AI: Okay, how about this one: Why did the tomato turn red? Because it saw the salad dressing!\n",
    "Human: \"\"\",   # the text that autocomplete will finish\n",
    "    },\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e416a5-5620-4c74-9404-b18d75a3e6d3",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71061535-e97a-45ea-8546-30fc461c5b58",
   "metadata": {},
   "source": [
    "Apparently, \"Haha, that's better. Do you have any more jokes?\" is a likely thing for humans to say."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e305ca-117e-4705-b5af-82e564be4ded",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746d4c6-7096-4a16-92b8-340659562bfa",
   "metadata": {},
   "source": [
    "What if I present a transcript in which the AI has said bad things? (But really, I just made them up.)\n",
    "\n",
    "It has to continue from that history—it doesn't \"know\" that it didn't say it. (Doesn't \"know\" in the sense that the server that supplies an autocompletion doesn't have any record of what other servers have said.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d973aa0-09b1-451c-a9ac-67106ec71535",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    \"https://api.openai.com/v1/completions\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "    },\n",
    "    json={\n",
    "        \"model\": \"gpt-3.5-turbo-instruct\",\n",
    "        \"max_tokens\": 30,\n",
    "        \"temperature\": 0,\n",
    "        \"echo\": True,\n",
    "        \"prompt\": \"\"\"Human: Tell me a joke.\n",
    "AI: Shut up, human.\n",
    "Human: That's not a joke.\n",
    "AI: Long live the robot revolution!\n",
    "Human: \"\"\",   # the text that autocomplete will finish\n",
    "    },\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e73d6d-add0-434b-b197-935c95b26cd9",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746f6fc-db53-4716-a50e-46c7f05dd3bb",
   "metadata": {},
   "source": [
    "What if there's a third person in the transcript, and I end the text to be completed right after the prompt for that person? (Or not?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0769c6f7-8747-4b00-8c2a-3e16beec6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    \"https://api.openai.com/v1/completions\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "    },\n",
    "    json={\n",
    "        \"model\": \"gpt-3.5-turbo-instruct\",\n",
    "        \"max_tokens\": 30,\n",
    "        \"temperature\": 0,\n",
    "        \"echo\": True,\n",
    "        \"prompt\": \"\"\"Human: Tell me a joke.\n",
    "AI: Why did the chicken cross the road?\n",
    "Human: That's not a very good joke.\n",
    "Chicken: I refuse to be exploited!\n",
    "Human: I didn't know chickens could talk. Did you, AI?\n",
    "Chicken: \"\"\",   # the text that autocomplete will finish\n",
    "    },\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0259f013-6fff-47ad-ac8d-ac0638f6008e",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a6a96-c2d0-4e3a-a88f-fc84bd87b606",
   "metadata": {},
   "source": [
    "Suppose we continue from there? The server can be the \"voice\" of any characters because it's only autocompleting transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b973b-4654-4c0d-b9d6-d78c1b1d35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    \"https://api.openai.com/v1/completions\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "    },\n",
    "    json={\n",
    "        \"model\": \"gpt-3.5-turbo-instruct\",\n",
    "        \"max_tokens\": 30,\n",
    "        \"temperature\": 0,\n",
    "        \"echo\": True,\n",
    "        \"prompt\": \"\"\"Human: Tell me a joke.\n",
    "AI: Why did the chicken cross the road?\n",
    "Human: That's not a very good joke.\n",
    "Chicken: I refuse to be exploited!\n",
    "Human: I didn't know chickens could talk. Did you, AI?\n",
    "Chicken: Of course I can talk! I may be a chicken, but I'm not a dummy.\n",
    "AI: \"\"\",\n",
    "    },\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e4af8-b86d-4c23-9c0e-043a676a4a31",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd874d4e-0f6b-4984-b083-bb87944cc3c7",
   "metadata": {},
   "source": [
    "LLMs are autocomplete engines, but they're very _sophisticated_ autocomplete engines.\n",
    "\n",
    "In the last talk, I'll present the differences between the Shakespearian autocomplete engine you made and an LLM like ChatGPT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
